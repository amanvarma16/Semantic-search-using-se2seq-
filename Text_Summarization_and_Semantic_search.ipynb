{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization and Semantic search.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanvarma16/Semantic-search-using-se2seq-/blob/master/Text_Summarization_and_Semantic_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rh56pzZZNfj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae995a07-0330-4048-f63a-502f107d747a"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q ktext\n",
        "!pip install -q annoy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "450oca8uNyiD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1df9f2af-f962-4d32-fc45-2040e910fd01"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "from urllib.request import urlopen\n",
        "\n",
        "from annoy import AnnoyIndex\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, LSTM, GRU, Embedding, Lambda, BatchNormalization\n",
        "from keras.models import load_model, Model\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import get_file, to_categorical\n",
        "from ktext.preprocess import processor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYIoUsDbOCSl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2E1QmszOP4V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Datasets for code snippets and Language Model "
      ]
    },
    {
      "metadata": {
        "id": "bPaZyRJ5OfoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "166dd7c0-6f84-428a-9f5f-c327e0bf7a0e"
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
        "!unzip -o conala-corpus-v1.1.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-28 05:35:39--  http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
            "Resolving www.phontron.com (www.phontron.com)... 208.113.196.149\n",
            "Connecting to www.phontron.com (www.phontron.com)|208.113.196.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52105440 (50M) [application/zip]\n",
            "Saving to: ‘conala-corpus-v1.1.zip’\n",
            "\n",
            "conala-corpus-v1.1. 100%[===================>]  49.69M  13.1MB/s    in 4.3s    \n",
            "\n",
            "2019-01-28 05:35:44 (11.5 MB/s) - ‘conala-corpus-v1.1.zip’ saved [52105440/52105440]\n",
            "\n",
            "Archive:  conala-corpus-v1.1.zip\n",
            "   creating: conala-corpus/\n",
            "  inflating: conala-corpus/conala-mined.jsonl  \n",
            "  inflating: conala-corpus/conala-train.json  \n",
            "  inflating: conala-corpus/conala-test.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vdY4uTSkO3Bm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('conala-corpus/conala-mined.jsonl' , 'r') as f :\n",
        "  lines = [json.loads(line) for line in f.readlines()]\n",
        "  source_docs = [line['snippet'] for line in lines]\n",
        "  target_docs = [line ['intent'] for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oM842FyJRQ68",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('conala-corpus/conala-train.json', 'r') as f:\n",
        "    lines = json.load(f)\n",
        "train_source_docs = [line['snippet'] for line in lines]\n",
        "train_target_docs = [line['intent'] for line in lines]\n",
        "test_docs = [line['rewritten_intent'] for line in lines if line['rewritten_intent']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYy9A7gORXJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('conala-corpus/conala-test.json', 'r') as f:\n",
        "    lines = json.load(f)\n",
        "test_source_docs = [line['snippet'] for line in lines]\n",
        "test_target_docs = [line['intent'] for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fEPk8tpRaeH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Subset\n",
        "\n",
        "source_docs = source_docs[:200000]\n",
        "target_docs = target_docs[:200000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6W04oFDWR1vY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "124m38ruSrDv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pre-Processing\n",
        "\n",
        "Cleaning - Lowercase, punctuation, \n",
        "\n",
        "Tokenize - Split each document into a list of words\n",
        "\n",
        "Build Vocab - ids for tokens\n",
        "\n",
        "Padding - To make the length of the documents uniform\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G9n4tCMPTn_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "50a913b6-c1e9-4f6c-b296-0be0c0141939"
      },
      "cell_type": "code",
      "source": [
        "for x in source_docs[:10]:\n",
        "  print (x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sorted(l, key=lambda x: (-int(x[1]), x[0]))\n",
            "[int(x) for x in str(num)]\n",
            "c.decode('unicode_escape')\n",
            "parser.add_argument('-t', dest='table', help='', nargs='+')\n",
            "datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M:%SZ')\n",
            "np.array(x._data).reshape(x.size[::-1]).T\n",
            "soup.get_text().replace('\\n', '\\n\\n')\n",
            "re.sub('(?<!\\\\S)((\\\\S+)(?:\\\\s+\\\\2))(?:\\\\s+\\\\2)+(?!\\\\S)', '\\\\1', s)\n",
            "mylist.sort(key=lambda d: (d['weight'], d['factor']))\n",
            "itertools.combinations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LjrDk-FwUQFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5ccfe056-f9ed-4494-9ef3-3d5ee3b1f57e"
      },
      "cell_type": "code",
      "source": [
        "for x in target_docs[:10]:\n",
        "  print(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sort a nested list by two elements\n",
            "converting integer to list in python\n",
            "Converting byte string in unicode string\n",
            "List of arguments with argparse\n",
            "How to convert a Date string to a DateTime object?\n",
            "How to efficiently convert Matlab engine arrays to numpy ndarray?\n",
            "Converting html to text with Python\n",
            "regex for repeating words in a string in Python\n",
            "Ordering a list of dictionaries in python\n",
            "Two Combination Lists from One List\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-IXIfW-vVjjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "65dcb965-0ba9-4bd3-8ffd-b757dceb65c1"
      },
      "cell_type": "code",
      "source": [
        "proc = processor(hueristic_pct_padding=.7, keep_n=5000)\n",
        "vecs = proc.fit_transform(target_docs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:Setting maximum document length to 10 based upon hueristic of 0.7 percentile.\n",
            " See full histogram by insepecting the `document_length_stats` attribute.\n",
            "WARNING:root:(1/2) done. 41 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 1 sec\n",
            "WARNING:root:Finished parsing 200,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 1 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "30qfn8c-ZHJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c624e898-19d8-4fcb-de89-19739f2f25ae"
      },
      "cell_type": "code",
      "source": [
        "print ('orginal list : ',target_docs[0])\n",
        "\n",
        "print ('transformed list :', vecs[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "orginal list :  Sort a nested list by two elements\n",
            "transformed list : [  0   0   0 135   2 145  11  45  37  62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "26vsPgzmZu1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "567c22c9-2602-4cf5-8fcf-e47120bb5271"
      },
      "cell_type": "code",
      "source": [
        "proc.token_count_pandas().head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>103489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>python</th>\n",
              "      <td>96545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>94008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>92498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>how</th>\n",
              "      <td>70208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         count\n",
              "a       103489\n",
              "python   96545\n",
              "in       94008\n",
              "to       92498\n",
              "how      70208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "BE0YzDz5Z6g8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "35802888-c376-4037-d0b4-7e3999a985a1"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = max(proc.id2token.keys()) + 1\n",
        "max_length = proc.padding_maxlen\n",
        "\n",
        "print('vocab size: ', vocab_size)\n",
        "print('max length allowed for documents: ', max_length)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  5002\n",
            "max length allowed for documents:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "voe1KpXXaLnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "761bda13-2eea-4d2f-e01c-c814aad684b1"
      },
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "for arr in tqdm(vecs):\n",
        "    non_zero = (arr != 0).argmax()\n",
        "    for i in range(non_zero, len(arr)):\n",
        "        sequences.append(arr[:i+1])\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "\n",
        "print (sequences[2])\n",
        "\n",
        "print (X)\n",
        "\n",
        "print (y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200000/200000 [00:01<00:00, 153647.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0 135   2 145]\n",
            "[[   0    0    0 ...    0    0    0]\n",
            " [   0    0    0 ...    0    0  135]\n",
            " [   0    0    0 ...    0  135    2]\n",
            " ...\n",
            " [   0    0    0 ...    0    0   13]\n",
            " [   0    0    0 ...    0   13 1330]\n",
            " [   0    0    0 ...   13 1330    8]]\n",
            "[ 135    2  145 ... 1330    8  194]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HXJw4eB4eo7S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "7e2bd660-f451-45e5-99fe-f15a22c9510a"
      },
      "cell_type": "code",
      "source": [
        "i = Input(shape = (max_length-1,))\n",
        "x= Embedding(vocab_size,256,input_length = max_length-1) (i)\n",
        "x = LSTM(256, return_sequences = True)(x)\n",
        "last_timestep = Lambda(lambda x : x[:,-1,:]) (x)\n",
        "last_timestep = Dense(vocab_size, activation ='softmax') (last_timestep)\n",
        "model = Model(i,last_timestep)\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 9)                 0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 9, 256)            1280512   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 9, 256)            525312    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5002)              1285514   \n",
            "=================================================================\n",
            "Total params: 3,091,338\n",
            "Trainable params: 3,091,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oBbu4vG4qxoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a15a2724-3774-4c35-a101-a75e78bd49a7"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=1, batch_size=500, validation_split=0.1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1466839 samples, validate on 162983 samples\n",
            "Epoch 1/1\n",
            "1466839/1466839 [==============================] - 142s 97us/step - loss: 5.2690 - acc: 0.1967 - val_loss: 4.8326 - val_acc: 0.2431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "44SIAbFxr6X9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pfD_gDuHuMNv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using a cached model\n"
      ]
    },
    {
      "metadata": {
        "id": "PBu5v9MJuOzQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b836bc8-38d3-4cd2-d984-e60331630de1"
      },
      "cell_type": "code",
      "source": [
        "assert len(target_docs) == 200000, 'target_docs should be truncated to the first 200k rows to use the cached model.'\n",
        "\n",
        "fname = get_file(fname='kdd_lm_v2.h5', origin='https://storage.googleapis.com/kdd-seq2seq-2018/kdd_lm_v2.h5', )\n",
        "model = load_model(fname)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/kdd-seq2seq-2018/kdd_lm_v2.h5\n",
            "31850496/31848704 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9gIYzr2JuShT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_seq(model, proc, n_words, seed_text):\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_words):\n",
        "        vec = proc.transform([in_text])[:,1:]\n",
        "        index = np.argmax(model.predict(vec, verbose=0), axis=1)[0]\n",
        "        out_word = ''\n",
        "        if index == 1:\n",
        "            out_word = '_unk_'\n",
        "        else:\n",
        "            out_word = proc.id2token[index]\n",
        "        in_text += ' ' + out_word\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mnezw39MwTag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2e63df5-9058-43bc-e9be-2bc25587112d"
      },
      "cell_type": "code",
      "source": [
        "generate_seq(model,proc,max_length,'string')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'string substitution performance in python logging and _unk_ in to c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "d-Evx9Kawhn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0R1Vg7Rw8Cz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## We trained our language model to learn representations of sentences in our corpus"
      ]
    },
    {
      "metadata": {
        "id": "IqW6l3a-yFab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8wD_tRKE6m13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "2596496e-ff83-4228-84a7-1cf7914db0ca"
      },
      "cell_type": "code",
      "source": [
        "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
        "print('input sequence: ', input_sequence, '\\n\\nhidden states:\\n')\n",
        "vec = proc.transform([input_sequence])[:,1:]\n",
        "embedding_model.predict(vec)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input sequence:  urlencode a querystring 'string_of_characters_like_these:$#@=?%^Q^$' in python 2 \n",
            "\n",
            "hidden states:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.        ,  0.        , -0.        , ...,  0.        ,\n",
              "         -0.41064757,  0.4782344 ],\n",
              "        [ 0.3200685 ,  0.7614881 , -0.00804727, ...,  0.5636641 ,\n",
              "         -0.07097925,  0.90842754],\n",
              "        [ 0.34599552,  0.9610021 ,  0.761538  , ...,  0.18917751,\n",
              "          0.69573283,  0.98709613],\n",
              "        ...,\n",
              "        [ 0.        ,  0.        , -0.        , ...,  0.        ,\n",
              "         -0.        ,  0.        ],\n",
              "        [ 0.04048524,  0.        ,  0.07371317, ...,  0.6745981 ,\n",
              "         -0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        , -0.7506168 , ...,  0.        ,\n",
              "         -0.        ,  0.        ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "4-0vqiXq6qDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "haQjC19y0v9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_vecs = proc.transform(test_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rLeLELte0v9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_states = embedding_model.predict(test_vecs[:, 1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k6opyQrT7xTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mean_vecs = np.mean(hidden_states, axis=1)\n",
        "max_vecs = np.max(hidden_states, axis=1)\n",
        "sum_vecs = np.sum(hidden_states, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "38DAFEGi70Oz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "95061928-8cb1-4d60-b624-5efbee1fc9d8"
      },
      "cell_type": "code",
      "source": [
        "print (hidden_states[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.          0.32860067 -0.         ...  0.          0.\n",
            "   0.7615874 ]\n",
            " [-0.6193741   0.         -0.         ...  0.         -0.0956229\n",
            "   0.9640264 ]\n",
            " [-0.         -0.         -0.7721795  ...  0.         -0.\n",
            "   0.9950546 ]\n",
            " ...\n",
            " [-0.73660165  0.         -0.         ...  0.         -0.18717118\n",
            "   0.38312688]\n",
            " [-0.          0.         -0.25832775 ...  0.         -0.73036677\n",
            "   0.99999976]\n",
            " [-0.          0.         -0.99879473 ...  0.         -0.38547844\n",
            "   0.99999994]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aUBfNyFA8BCK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45DAUyHX8nwR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build Vector Indices. Use Annoy"
      ]
    },
    {
      "metadata": {
        "id": "fCXWWQmD8rz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "613ebf32-d2a0-452b-8bdc-0bbf14d720b0"
      },
      "cell_type": "code",
      "source": [
        "dimension = hidden_states.shape[-1]\n",
        "index = AnnoyIndex(dimension)\n",
        "for i, v in enumerate(sum_vecs):\n",
        "    index.add_item(i, v)\n",
        "index.build(10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "Bwei8pME8uVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "13c6ff3d-7da3-4099-84ce-0d3a3583f8b2"
      },
      "cell_type": "code",
      "source": [
        "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
        "print('Query: ', input_sequence)\n",
        "\n",
        "vec = proc.transform([input_sequence])[:,1:]\n",
        "vec = np.sum(embedding_model.predict(vec), axis=1)\n",
        "ids, _ = index.get_nns_by_vector(vec.T, 10, include_distances=True)\n",
        "\n",
        "print('\\nSearch Results:')\n",
        "[test_docs[i] for i in ids][1:]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query:  Convert ascii value 'P' to binary\n",
            "\n",
            "Search Results:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"convert ascii value 'a' to int\",\n",
              " 'Convert hex string `hexString` to int',\n",
              " 'convert hex string `s` to decimal',\n",
              " 'Convert hex string `s` to integer',\n",
              " 'Convert hex string \"deadbeef\" to integer',\n",
              " 'Convert hex string \"0xa\" to integer',\n",
              " 'convert hex string \"0xff\" to decimal',\n",
              " 'convert hex string \"FFFF\" to decimal',\n",
              " \"convert hex string '0xdeadbeef' to decimal\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "T8z9t3Cu9MZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nt7rQIOx-iqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SEQ 2 SEQ MODEL\n",
        "\n",
        "## We'll use it for a creative task : Given a snippet of code, it should be able to generate a description of that code.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FNdYr7Lagzun",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll use the simplest of the seq2seq model, an encoder-decoder network using RNNs"
      ]
    },
    {
      "metadata": {
        "id": "cEcMkBoA-1Xx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f2948fbc-ae5c-4a6f-9c09-b3387957fd25"
      },
      "cell_type": "code",
      "source": [
        "print('source (code input): ', source_docs[2])\n",
        "print('target (description output): ', target_docs[2])\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source (code input):  c.decode('unicode_escape')\n",
            "target (description output):  Converting byte string in unicode string\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kCZXA-2-3S80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "38bcff50-1681-4d05-bafc-275ec8623464"
      },
      "cell_type": "code",
      "source": [
        "target_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=12000, padding ='post')\n",
        "target_vecs = target_proc.fit_transform(target_docs)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:Setting maximum document length to 15 based upon hueristic of 0.7 percentile.\n",
            " See full histogram by insepecting the `document_length_stats` attribute.\n",
            "WARNING:root:(1/2) done. 43 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 1 sec\n",
            "WARNING:root:Finished parsing 200,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 1 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QrZY6wZv3bV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "47807fb3-2933-4269-fd3a-5afb9ac2a973"
      },
      "cell_type": "code",
      "source": [
        "source_proc = processor(hueristic_pct_padding=.7, keep_n=15000)\n",
        "source_vecs = source_proc.fit_transform(source_docs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:Setting maximum document length to 10 based upon hueristic of 0.7 percentile.\n",
            " See full histogram by insepecting the `document_length_stats` attribute.\n",
            "WARNING:root:(1/2) done. 39 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 1 sec\n",
            "WARNING:root:Finished parsing 200,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 1 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "abp1PeHR3oKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_input_data = source_vecs\n",
        "encoder_seq_len = encoder_input_data.shape[1]\n",
        "\n",
        "decoder_input_data = target_vecs[:, :-1]\n",
        "decoder_target_data = target_vecs[:, 1:]\n",
        "\n",
        "num_encoder_tokens = max(source_proc.id2token.keys()) + 1\n",
        "num_decoder_tokens = max(target_proc.id2token.keys()) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mT1ErEp74Cx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6361721d-3094-4576-a31e-3311c164ccdc"
      },
      "cell_type": "code",
      "source": [
        "print (encoder_input_data [2])\n",
        "print (decoder_input_data [2])\n",
        "\n",
        "print (decoder_target_data[2])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0  20 259 522 633]\n",
            "[  2 164 514  17   6 144  17   3   0   0   0   0   0   0]\n",
            "[164 514  17   6 144  17   3   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_s3XacKP4G4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32aWAWE36IBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Encoder Model"
      ]
    },
    {
      "metadata": {
        "id": "rrbmJ9vr6OT9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The role of the encoder is to extract features and generate a representation of the input sequence, which in this case is a snippet of code."
      ]
    },
    {
      "metadata": {
        "id": "wprku38P6PDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_emb_dim=512\n",
        "hidden_state_dim=1024\n",
        "encoder_seq_len=encoder_seq_len\n",
        "num_encoder_tokens=num_encoder_tokens\n",
        "num_decoder_tokens=num_decoder_tokens\n",
        "\n",
        "encoder_inputs = Input(shape=(encoder_seq_len,), name='Encoder-Input')\n",
        "x = Embedding(num_encoder_tokens, word_emb_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "_, state_h = GRU(hidden_state_dim, return_state=True, name='Encoder-Last-GRU', dropout=.5)(x)\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b8m6NRhR6ZjD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "58a6581b-999f-4ed2-fcdf-9bc75b8aca3c"
      },
      "cell_type": "code",
      "source": [
        "encoder_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Encoder-Input (InputLayer)   (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "Body-Word-Embedding (Embeddi (None, 10, 512)           7681024   \n",
            "_________________________________________________________________\n",
            "Encoder-Batchnorm-1 (BatchNo (None, 10, 512)           2048      \n",
            "_________________________________________________________________\n",
            "Encoder-Last-GRU (GRU)       [(None, 1024), (None, 102 4721664   \n",
            "=================================================================\n",
            "Total params: 12,404,736\n",
            "Trainable params: 12,403,712\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vq23U-wx6gKp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43fK9M0U61MF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decoder Model"
      ]
    },
    {
      "metadata": {
        "id": "BwFESuB6624e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,), name = 'Decoder-Input')\n",
        "dec_emb = Embedding(num_decoder_tokens, word_emb_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "decoder_gru = GRU(hidden_state_dim, return_state=True, return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zpTDn_kf766W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRLAl72H8G02",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sequence to sequence model\n",
        "We can connect the encoder and decoder together to create the sequence to sequence model.\n",
        "\n",
        "[ ]\n",
        "\n",
        "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "metadata": {
        "id": "r07gw_7Y8LK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNMNsixd8IOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "83a07753-0620-442e-a4fe-cf615985b983"
      },
      "cell_type": "code",
      "source": [
        "seq2seq_model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Word-Embedding (Embeddi (None, None, 512)    5846528     Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Input (InputLayer)      (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-1 (BatchNorma (None, None, 512)    2048        Decoder-Word-Embedding[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Model (Model)           (None, 1024)         12404736    Encoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-GRU (GRU)               [(None, None, 1024), 4721664     Decoder-Batchnorm-1[0][0]        \n",
            "                                                                 Encoder-Model[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-2 (BatchNorma (None, None, 1024)   4096        Decoder-GRU[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Final-Output-Dense (Dense)      (None, None, 11419)  11704475    Decoder-Batchnorm-2[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 34,683,547\n",
            "Trainable params: 34,679,451\n",
            "Non-trainable params: 4,096\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pi4MkIok8RZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "epochs = 1\n",
        "\n",
        "seq2seq_model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = seq2seq_model.fit([encoder_input_data, decoder_input_data],\n",
        "                            np.expand_dims(decoder_target_data, -1),\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPMH4FNN8t9q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/kdd-seq2seq-2018/kdd_seq2seq_weights.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvICZA828cfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq2seq_model.load_weights('kdd_seq2seq_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2spHcZM817a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eX9c3k7zA7d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now Extract Encoder and Decoder from the model layers.\n",
        "Encoder - To create embeddings for the raw input text\n",
        "\n",
        "Decoder - To generate descriptions for the code snippets "
      ]
    },
    {
      "metadata": {
        "id": "UorGtTOCBkEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_decoder_model(model):\n",
        "    latent_dim = model.get_layer('Encoder-Model').output_shape[-1]\n",
        "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
        "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
        "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
        "    decoder_model = Model([decoder_inputs, gru_inference_state_input], [dense_out, gru_state_out])\n",
        "    return decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9mo-IEbDU_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### We can extract Encoder as a general purpose feature Extractor"
      ]
    },
    {
      "metadata": {
        "id": "TWwOPS_CDf5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "89147b90-84a4-48b7-bb9e-a545f8115885"
      },
      "cell_type": "code",
      "source": [
        "encoder_model = seq2seq_model.get_layer('Encoder-Model')\n",
        "for layer in encoder_model.layers :\n",
        "  layer.trainable = False\n",
        "  \n",
        "  \n",
        "decoder_model = extract_decoder_model(seq2seq_model)\n",
        "decoder_model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Word-Embedding (Embeddi (None, None, 512)    5846528     Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-1 (BatchNorma (None, None, 512)    2048        Decoder-Word-Embedding[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "hidden_state_input (InputLayer) (None, 1024)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-GRU (GRU)               [(None, None, 1024), 4721664     Decoder-Batchnorm-1[1][0]        \n",
            "                                                                 hidden_state_input[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-2 (BatchNorma (None, None, 1024)   4096        Decoder-GRU[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Final-Output-Dense (Dense)      (None, None, 11419)  11704475    Decoder-Batchnorm-2[1][0]        \n",
            "==================================================================================================\n",
            "Total params: 22,278,811\n",
            "Trainable params: 22,275,739\n",
            "Non-trainable params: 3,072\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BnRGj7qjEJy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPZH7XsQFQwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prediction using our decoder model for raw input in test_source"
      ]
    },
    {
      "metadata": {
        "id": "jJp0P5dxFanF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2e65ba8b-01a3-49e1-96ba-58da3d82ea23"
      },
      "cell_type": "code",
      "source": [
        "i = random.randint(0,len(test_source_docs))\n",
        "\n",
        "max_len = target_proc.padding_maxlen\n",
        "raw_input_text = test_source_docs[i]\n",
        "\n",
        "raw_tokenized = source_proc.transform([raw_input_text])\n",
        "encoding = encoder_model.predict(raw_tokenized)\n",
        "original_encoding = encoding\n",
        "state_value = np.array(target_proc.token2id['_start_']).reshape(1, 1)\n",
        "\n",
        "decoded_sentence = []\n",
        "stop_condition = False\n",
        "while not stop_condition:\n",
        "    preds, st = decoder_model.predict([state_value, encoding])\n",
        "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "    pred_word_str = target_proc.id2token[pred_idx]\n",
        "\n",
        "    if pred_word_str == '_end_' or len(decoded_sentence) >= max_len:\n",
        "        stop_condition = True\n",
        "        break\n",
        "    decoded_sentence.append(pred_word_str)\n",
        "\n",
        "    # update the decoder for the next word\n",
        "    encoding = st\n",
        "    state_value = np.array(pred_idx).reshape(1, 1)\n",
        "\n",
        "print('sample code from test set:\\n------------------------\\n', raw_input_text)\n",
        "print('\\nground truth:\\n------------------------\\n', test_target_docs[i])\n",
        "print('\\npredicted description:\\n------------------------')\n",
        "print(' '.join(decoded_sentence))\n",
        "\n",
        " \n",
        " "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample code from test set:\n",
            "------------------------\n",
            " re.findall('n(?<=[^n]n)n+(?=[^n])(?i)', s)\n",
            "\n",
            "ground truth:\n",
            "------------------------\n",
            " Regex, find pattern only in middle of string\n",
            "\n",
            "predicted description:\n",
            "------------------------\n",
            "dictionary bail arange spesific rich pdf dataframe parenthesis cyrillic startmenu works cryptographic notebooks dependencies potential\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZcqJD1V0KGhB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vsiUw8tROLiy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate Embeddings\n",
        "\n",
        "We need two embeddings\n",
        "\n",
        "Embeddings for the code snippets, from the seq2seq encoder.\n",
        "\n",
        "Embeddings for the docstrings, from the language model"
      ]
    },
    {
      "metadata": {
        "id": "8_omYeH_OQb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7Al-Ej9OWX7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Embedding for code snippets"
      ]
    },
    {
      "metadata": {
        "id": "kgC54jyxOZ_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_source_emb = encoder_model.predict(source_proc.transform(source_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgyUCrZIOrlB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2gTfc6vTOuR5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Embeddings for target docs which are docstrings, natural language "
      ]
    },
    {
      "metadata": {
        "id": "O6O0MckfO2q-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_target_vecs = proc. transform(target_docs)\n",
        "hidden_states = embedding_model.predict(train_target_vecs[:,1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ooIdt2RP0b8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPLiSovbP1Ij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Summarize hidden states from embedding model"
      ]
    },
    {
      "metadata": {
        "id": "Vu_JUN7GP4aa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mean_vecs = np.mean(hidden_states,axis=1)\n",
        "max_vecs = np.max(hidden_states, axis =1)\n",
        "sum_vecs = np.sum(hidden_states, axis =1)\n",
        "\n",
        "train_target_emb = mean_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2hKSOuRQmF5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3eb4073-91ee-46f3-f354-9ea3127924c6"
      },
      "cell_type": "code",
      "source": [
        "print('source embedding shape on training set: ', train_source_emb.shape)\n",
        "print('target embedding shape on training set: ', train_target_emb.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source embedding shape on training set:  (200000, 1024)\n",
            "target embedding shape on training set:  (200000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L0TF8FhRQupT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I4vQB-92VGbL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construct a Joint Vector Space (Semantic Code Search)\n",
        "Right now we have a way of representing:\n",
        "\n",
        "a blob of code as a vector using the encoder of the sequence-to-sequence model, and\n",
        "the code descriptions as a vector using the language model.\n",
        "However, these two vector spaces are not related to eachother. It can be useful to project the vectors for code and descriptions into the same space so that we can search code with natural language."
      ]
    },
    {
      "metadata": {
        "id": "H6x9pIAUV0TQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "54de80ef-c72e-4b8b-ee81-ee09363b5451"
      },
      "cell_type": "code",
      "source": [
        "#Most of the pieces for this step come from prior steps in this tutorial. In this step, we will fine-tune the seq2seq model to predict docstring embeddings instead of docstrings.\n",
        "\n",
        "\n",
        "inp = Input(shape=(train_source_emb.shape[1],))\n",
        "x = Dense(train_target_emb.shape[1], use_bias=False)(inp)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dense(train_target_emb.shape[1])(x)\n",
        "modal_model = Model([inp], x)\n",
        "modal_model.summary()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               204800    \n",
            "=================================================================\n",
            "Total params: 204,800\n",
            "Trainable params: 204,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3jlYi2drW6kO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "b799827e-9d78-4c05-d1c8-b904ada986f4"
      },
      "cell_type": "code",
      "source": [
        "modal_model.compile(optimizer=optimizers.Nadam(lr=0.0001), loss='cosine_proximity', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 1200\n",
        "epochs = 20\n",
        "history = modal_model.fit([train_source_emb], train_target_emb,\n",
        "                          batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 180000 samples, validate on 20000 samples\n",
            "Epoch 1/20\n",
            "180000/180000 [==============================] - 2s 12us/step - loss: -0.5107 - acc: 0.0215 - val_loss: -0.6320 - val_acc: 0.0316\n",
            "Epoch 2/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.6623 - acc: 0.0384 - val_loss: -0.6692 - val_acc: 0.0448\n",
            "Epoch 3/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.6866 - acc: 0.0488 - val_loss: -0.6871 - val_acc: 0.0518\n",
            "Epoch 4/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7000 - acc: 0.0560 - val_loss: -0.6990 - val_acc: 0.0584\n",
            "Epoch 5/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7085 - acc: 0.0599 - val_loss: -0.7059 - val_acc: 0.0615\n",
            "Epoch 6/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7138 - acc: 0.0630 - val_loss: -0.7104 - val_acc: 0.0657\n",
            "Epoch 7/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7175 - acc: 0.0658 - val_loss: -0.7137 - val_acc: 0.0684\n",
            "Epoch 8/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7204 - acc: 0.0681 - val_loss: -0.7163 - val_acc: 0.0704\n",
            "Epoch 9/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7229 - acc: 0.0704 - val_loss: -0.7197 - val_acc: 0.0730\n",
            "Epoch 10/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7255 - acc: 0.0714 - val_loss: -0.7218 - val_acc: 0.0723\n",
            "Epoch 11/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7271 - acc: 0.0729 - val_loss: -0.7232 - val_acc: 0.0737\n",
            "Epoch 12/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7283 - acc: 0.0741 - val_loss: -0.7243 - val_acc: 0.0752\n",
            "Epoch 13/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7294 - acc: 0.0760 - val_loss: -0.7251 - val_acc: 0.0764\n",
            "Epoch 14/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7303 - acc: 0.0769 - val_loss: -0.7259 - val_acc: 0.0776\n",
            "Epoch 15/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7310 - acc: 0.0780 - val_loss: -0.7266 - val_acc: 0.0784\n",
            "Epoch 16/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7317 - acc: 0.0783 - val_loss: -0.7272 - val_acc: 0.0790\n",
            "Epoch 17/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7323 - acc: 0.0791 - val_loss: -0.7277 - val_acc: 0.0804\n",
            "Epoch 18/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7329 - acc: 0.0798 - val_loss: -0.7282 - val_acc: 0.0799\n",
            "Epoch 19/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7334 - acc: 0.0803 - val_loss: -0.7285 - val_acc: 0.0812\n",
            "Epoch 20/20\n",
            "180000/180000 [==============================] - 1s 7us/step - loss: -0.7338 - acc: 0.0809 - val_loss: -0.7290 - val_acc: 0.0814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5AvVm5ezXvKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zaYryV3lYPLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## APPLICATION - Semantic search"
      ]
    },
    {
      "metadata": {
        "id": "7YAbTyE3YV7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Warning : Results of this portion are minimal and not very good."
      ]
    },
    {
      "metadata": {
        "id": "6v9KHARpYeMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AUaaVUFBY1HE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1) **Vectorize all of the code** :  We are going to push all the code through the encoder -> then modal_model to project the code into the same vector-space as our phrase embeddings."
      ]
    },
    {
      "metadata": {
        "id": "gMyaZI0LY8ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "376281bc-1be8-410e-fb4f-1fddab2564bf"
      },
      "cell_type": "code",
      "source": [
        "vec_code = modal_model.predict(encoder_model.predict(source_proc.transform(source_docs)))\n",
        "vec_code.shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "ljB3F4tbZcvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9HRgX0kZsFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2)** Place all vectorized code into search index** : We use Spotify's Annoy "
      ]
    },
    {
      "metadata": {
        "id": "2YvfzX3eZ8kr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6cecef1-6c71-4f54-dbfc-4756416dfc68"
      },
      "cell_type": "code",
      "source": [
        "dimension = vec_code.shape[-1]\n",
        "index = AnnoyIndex(dimension)\n",
        "\n",
        "for i,v in enumerate(vec_code):\n",
        "  index.add_item(i,v)\n",
        "index.build(10)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "qQM99I2Ganlq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EmMzftObayFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3) **Create function to vectorize queries **: we use embedding model for natural language to create sentence embeddings"
      ]
    },
    {
      "metadata": {
        "id": "IFf5Z19za9bF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize_queries(txt) :\n",
        "  vec = proc.transform([txt])[:,1:]\n",
        "  emb = np.mean(embedding_model.predict(vec),axis=1)\n",
        "  return emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HDao-0PRbyE5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OhzSnPAjcAPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Given a query, lookup the closest code in vector space**. . \n",
        "\n",
        "We can wrap everything in one function that:\n",
        "\n",
        "\n",
        "1)vectorizes a string query\n",
        "\n",
        "2)lookup the nearest vectorized code (which is in the same vector-space as the string query)\n",
        "\n",
        "3)retrieve the original code and display it for the user."
      ]
    },
    {
      "metadata": {
        "id": "LCDsgAjJcC3K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def search (inp) :\n",
        "  inp = vectorize_queries (inp)\n",
        "  ids, dist = index.get_nns_by_vector (inp.squeeze(),10,include_distances = True)\n",
        "  \n",
        "  for i, dist in zip(ids,dist) :\n",
        "    print(f'dist: {dist:.2f}\\n{source_docs[i]}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQulrC2fexxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SDyVgQIe0Rp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **The Demo**"
      ]
    },
    {
      "metadata": {
        "id": "24jq3jE9e7P_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "97486561-e00d-434a-933e-23c271fd7c05"
      },
      "cell_type": "code",
      "source": [
        "search('read csv file into pandas dataframe')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dist: 0.50\n",
            "df['Test'] = np.maximum(df['Open'], df['Close'])\n",
            "\n",
            "dist: 0.50\n",
            "with open('write.csv', 'a') as f:\n",
            "    df.to_csv(f, header=False, index=False)\n",
            "\n",
            "dist: 0.50\n",
            "data_df = pd.read_csv('data.csv')\n",
            "\n",
            "dist: 0.50\n",
            "import pandas as pd\n",
            "df = pd.DataFrame.from_csv('data.csv')\n",
            "result = df[(df.Sex == 'female') & (df.Survived == False)]\n",
            "\n",
            "dist: 0.50\n",
            "df = pd.DataFrame.from_csv('data.csv')\n",
            "\n",
            "dist: 0.50\n",
            "df = pd.read_csv('data.csv')\n",
            "\n",
            "dist: 0.51\n",
            "df.to_csv('data.csv')\n",
            "\n",
            "dist: 0.51\n",
            "resp = make_response(df.to_csv())\n",
            "\n",
            "dist: 0.51\n",
            "titanic = pd.read_csv('titanic_data.csv')\n",
            "\n",
            "dist: 0.51\n",
            "df = pandas.read_csv('Yourfile.csv')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BaP1axJUfACC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "15ea26c0-efdd-4770-d2da-73700440ff5c"
      },
      "cell_type": "code",
      "source": [
        "search('sort list by descending order')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dist: 0.54\n",
            "match = re.match('\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}Z', date_string)\n",
            "\n",
            "dist: 0.55\n",
            "d = {}\n",
            "for t in l:\n",
            "    d.setdefault(t[0], {})[t[1]] = t[2]\n",
            "\n",
            "dist: 0.55\n",
            "\"\"\"-\"\"\".join(a + b for a, b in zip(t, t))\n",
            "\n",
            "dist: 0.55\n",
            "t2 = [(a + b) for a, b in zip(t, t[1:])]\n",
            "t2.append(t[0] + t[-1])\n",
            "\n",
            "dist: 0.55\n",
            "t2 = [(a + b) for a, b in zip(t, t[1:])]\n",
            "\n",
            "dist: 0.55\n",
            "def compare(s, t):\n",
            "    return Counter(s) == Counter(t)\n",
            "\n",
            "dist: 0.55\n",
            "d = {'A': 'a', 'B': {'C': 'c', 'D': 'd', 'E': 'e'}}\n",
            "\n",
            "dist: 0.55\n",
            "d = {'A': 'a', 'B': {'C': 'c', 'D': 'd', 'E': 'e'}}\n",
            "with open('result.yml', 'w') as yaml_file:\n",
            "    yaml.dump(d, yaml_file, default_flow_style=False)\n",
            "\n",
            "dist: 0.55\n",
            "d = {'A': 'a', 'B': {'C': 'c', 'D': 'd', 'E': 'e'}}\n",
            "with open('result.yaml', 'w') as f:\n",
            "    yaml.dump(d, f, default_flow_style=False)\n",
            "\n",
            "dist: 0.55\n",
            "d = {'A': 'a', 'B': {'C': 'c', 'D': 'd', 'E': 'e'}}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2DMrY7MPfOOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "2b51c764-3175-40cd-8cc2-506d331e695a"
      },
      "cell_type": "code",
      "source": [
        "search ('pandas dataframe')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dist: 0.86\n",
            "info = {}\n",
            "\n",
            "dist: 0.88\n",
            "super(programmers, self).info()\n",
            "\n",
            "dist: 0.88\n",
            "a = 42 * np.ones(16, dtype=np.float32)\n",
            "\n",
            "dist: 0.89\n",
            "def __init__(self, a=None, b=None, c=None):\n",
            "    self.a = a\n",
            "\n",
            "dist: 0.89\n",
            "a = np.zeros((3000, 300), dtype=np.float32)\n",
            "\n",
            "dist: 0.89\n",
            "np_data = np.zeros((10, 10), dtype=np.float32)\n",
            "\n",
            "dist: 0.89\n",
            "self.y = []\n",
            "\n",
            "dist: 0.89\n",
            "pygame.sprite.Sprite.__init__(self)\n",
            "\n",
            "dist: 0.89\n",
            "return pd.DataFrame(self.values.copy(), self.index.copy(), self.columns.copy())\n",
            "\n",
            "dist: 0.89\n",
            "def __init__(self):\n",
            "    self._data = {}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DSIlGULofi7_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}